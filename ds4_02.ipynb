{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit338dbb2cb8464efc9e0e4a7e5c7dfcb4",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis 4 - Lecture 2\n",
    "This lecture is about storing and restoring model, application o fthe checkpoints.\n",
    "\n",
    "We use validating sets, and evaluate our models on FashionMnist dataset.\n",
    "\n",
    "Also we will try the TransferLearning simple/educational example where we use one model trained on a dataset and retrained only the final layer for different dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/jplatos/2019-2020-da4/blob/master/ds4_02.ipynb)\n",
    "[Download from Github](https://raw.githubusercontent.com/jplatos/2019-2020-DA4/master/ds4_02.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np #numpy\n",
    "import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n",
    "import tensorflow.keras as keras # required for high level applications\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some utility functions\n",
    "Here is some functions we will use later several times\n",
    "* **show_history** - show history of the **fit** method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataset load\n",
    "Importig dataset **FashonMinst** a collection of images 28x28 grayscale of typical clothes and accesories.\n",
    "Dataset is:\n",
    "* downloaded\n",
    "* splitted into train and test set\n",
    "* converted from the range <0,255> into <0, 1>\n",
    "* *train* is splitted into *train* and *validation* set \n",
    "* class names are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is the basic dataset with handwritten digits\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x, test_x = train_x/255.0, test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boo']\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Show first 25 images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_x[i], cmap=plt.cm.gray)\n",
    "    plt.xlabel(class_names[train_y[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Definition of the model\n",
    "The base model is defined as *Sequential* with three layers, similarly to the previous lecture.\n",
    "\n",
    "Summarization of the model and compilation is done similarly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(class_count, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fit the model for defined number of epochs.\n",
    "Show the history of learning, evaluate the efficiency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vizualisation of the evaluation\n",
    "The function *predict* return the prediction for the defined input - a test set.\n",
    "\n",
    "The prediction in this case is a vector of probabilities foe each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "\n",
    "print(predictions[0], np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vizualization of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = np.zeros((class_count, class_count))\n",
    "for idx, pred in enumerate(predictions):\n",
    "    row = test_y[idx]\n",
    "    col = np.argmax(pred)\n",
    "    conf_matrix[row, col] += 1\n",
    "\n",
    "\n",
    "# print(conf_matrix)\n",
    "conf_matrix = normalize(conf_matrix, axis=1, norm='l1')\n",
    "# print(conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(class_count,class_count))\n",
    "\n",
    "plt.imshow(conf_matrix)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = plt.text(j, i, \"{:.2f}\".format(conf_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save and Load of the model\n",
    "The model I/O works as follows. The topology of the model is usually stored separately from the weights. The reason is that the topology is usually used once, but the weights are used frequently and their develop during the learning process. \n",
    "\n",
    "The topology is stored as JSON file format, as is demonstrated later in more human readable form. As you may see, the document os very simple and just summarize the layers and its types - app parameters set during the model creation.\n",
    "\n",
    "The weights are stored using the HDF5 file format a format developed for fast unlimited storage with portable and distributable capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = 'model.json'\n",
    "file_weight = 'model.h5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(file_model, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(file_weight)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```json\n",
    "{\n",
    "    \"class_name\": \"Sequential\",\n",
    "    \"config\": {\n",
    "        \"name\": \"sequential\",\n",
    "        \"layers\": [\n",
    "            {\n",
    "                \"class_name\": \"Flatten\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"flatten\",\n",
    "                    \"trainable\": true,\n",
    "                    \"batch_input_shape\": [\n",
    "                        null,\n",
    "                        28,\n",
    "                        28\n",
    "                    ],\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"data_format\": \"channels_last\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 32,\n",
    "                    \"activation\": \"relu\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense_1\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 10,\n",
    "                    \"activation\": \"softmax\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"keras_version\": \"2.2.4-tf\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading model\n",
    "The model is also loadable separatelly. Topology and weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The model need to be compiled to be applicable to the data.As you may see the summary of the model is the same as was for the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation of the model has the same results like the stored one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = loaded_model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tasks for Lecture \n",
    "1. THe task here will be focused on the usage of some [CallBacks](https://keras.io/callbacks/) from the keras API which is intended to monitor learning process. The major ones are *ModelCheckpoint* and *EarlyStopping*.\n",
    "2. [Transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is a method for reusing a prepared model from some type of dataset to new dataset. The usage of that model has many advantages. Lets go throug the link attached and read the description.\n",
    "   * The problem will be demonstrated in the MNIST dataset (from the previous lecture).\n",
    "   * The model trained on the FashionMnist will be reused on the Mnist dataset.\n",
    "   * The first layers will be deactivated for training and model will be compiled.\n",
    "   * The last layer will be trained on the new dataset and the result uses the knowledge from the previous learning.\n",
    "3. Desing a model which will be able to classify **FashionMnist** with accuracy higher than **92%**."
   ]
  }
 ]
}
